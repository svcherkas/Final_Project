{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Скрипт для обучения и сохранения модели классификации изображений\n",
    " с использованием библиотек TensorFlow и Keras.\n",
    "\n",
    " Описание:\n",
    " Этот скрипт предназначен для создания и обучения модели машинного обучения,\n",
    " способной классифицировать изображения фруктов и овощей. Модель основана на \n",
    " предварительно обученной сети MobileNetV2, что позволяет добиться хорошей \n",
    " точности классификации. Обученная модель сохраняется в файл 'my_classification_model.h5'\n",
    " для дальнейшего использования в других приложениях.\n",
    "\n",
    " Используемые библиотеки:\n",
    " - TensorFlow и Keras для создания и обучения модели.\n",
    " - ImageDataGenerator для аугментации и обработки изображений.\n",
    " - MobileNetV2 предоставляет предварительно обученную базовую сеть.\n",
    " - Другие библиотеки, такие как os, shutil и random, для управления данными и файлами.\n",
    "\n",
    "Шаги выполнения:\n",
    "1. Устанавливаются пути к папкам с данными (набором изображений фруктов и овощей).\n",
    "2. Задаются параметры обработки изображений, такие как размер изображений и аугментации.\n",
    "3. Создаются генераторы для данных обучения и валидации.\n",
    "4. Создается и обучается модель. Сначала загружается предварительно обученная сеть MobileNetV2,\n",
    "    затем к ней добавляются дополнительные слои для классификации.\n",
    "5. Модель компилируется с оптимизатором 'adam' и функцией потерь 'categorical_crossentropy'.\n",
    "6. Обучение модели выполняется в течение нескольких эпох с использованием генераторов данных.\n",
    "7. Обученная модель сохраняется в файл 'my_classification_model.h5' для будущего использования.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.python.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7769fffd",
   "metadata": {},
   "source": [
    "Выводим названия папок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c140876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Названия папок:\n",
      "apple\n",
      "banana\n",
      "beetroot\n",
      "bell pepper\n",
      "cabbage\n",
      "capsicum\n",
      "carrot\n",
      "cauliflower\n",
      "chilli pepper\n",
      "corn\n",
      "cucumber\n",
      "eggplant\n",
      "garlic\n",
      "ginger\n",
      "grapes\n",
      "jalepeno\n",
      "kiwi\n",
      "lemon\n",
      "lettuce\n",
      "mango\n",
      "onion\n",
      "orange\n",
      "paprika\n",
      "pear\n",
      "peas\n",
      "pineapple\n",
      "pomegranate\n",
      "potato\n",
      "raddish\n",
      "soy beans\n",
      "spinach\n",
      "sweetcorn\n",
      "sweetpotato\n",
      "tomato\n",
      "turnip\n",
      "watermelon\n"
     ]
    }
   ],
   "source": [
    "def list_folders(directory):\n",
    "    # Проверяем, что директория существует\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Директория '{directory}' не существует.\")\n",
    "        return\n",
    "\n",
    "    # Получаем все элементы в директории\n",
    "    items = os.listdir(directory)\n",
    "\n",
    "    # Фильтруем только папки\n",
    "    folders = [item for item in items if os.path.isdir(os.path.join(directory, item))]\n",
    "\n",
    "    # Выводим названия папок\n",
    "    print(\"Названия папок:\")\n",
    "    for folder in folders:\n",
    "        print(folder)\n",
    "\n",
    "# Указываем путь к директории\n",
    "directory_path = './train'\n",
    "\n",
    "# Выводим названия папок в заданной директории\n",
    "list_folders(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4599c3ef",
   "metadata": {},
   "source": [
    "Делаем разделение 1 раз, выделим из train  - val и test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3df4e886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Разделение изображений завершено.\n"
     ]
    }
   ],
   "source": [
    "# Путь к папке с данными\n",
    "data_folder = './train'\n",
    "\n",
    "# Пути к папкам val и test\n",
    "val_folder = './val/'\n",
    "test_folder = './test/'\n",
    "\n",
    "# Создание папок val и test, если они не существуют\n",
    "os.makedirs(val_folder, exist_ok=True)\n",
    "os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "# Проход по всем папкам внутри папки с данными\n",
    "for class_name in os.listdir(data_folder):\n",
    "    class_path = os.path.join(data_folder, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        # Получение списка изображений в текущей папке\n",
    "        images = [f for f in os.listdir(class_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        # Вычисление количества изображений для val и test (по 15% каждый)\n",
    "        num_val = int(0.15 * len(images))\n",
    "        num_test = int(0.15 * len(images))\n",
    "        \n",
    "        # Перемешивание изображений\n",
    "        random.shuffle(images)\n",
    "        \n",
    "        # Разделение на val, test и перемещение изображений\n",
    "        for i, image_name in enumerate(images):\n",
    "            source_path = os.path.join(class_path, image_name)\n",
    "            if i < num_val:\n",
    "                destination_folder = os.path.join(val_folder, class_name)\n",
    "            elif i < num_val + num_test:\n",
    "                destination_folder = os.path.join(test_folder, class_name)\n",
    "            else:\n",
    "                break  # Остальные изображения оставляем в папке train\n",
    "\n",
    "            os.makedirs(destination_folder, exist_ok=True)\n",
    "            destination_path = os.path.join(destination_folder, image_name)\n",
    "            shutil.move(source_path, destination_path)\n",
    "\n",
    "print('Разделение изображений завершено.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53834d90",
   "metadata": {},
   "source": [
    "Считаем сколько изображений "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef5ebbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее количество изображений в папке train: 2229\n",
      "Общее количество изображений в папке val: 443\n",
      "Общее количество изображений в папке test: 443\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_total_images(folder_path):\n",
    "    total_count = 0\n",
    "    for subdir in os.listdir(folder_path):\n",
    "        if os.path.isdir(os.path.join(folder_path, subdir)):\n",
    "            subdir_path = os.path.join(folder_path, subdir)\n",
    "            count = sum([len(files) for _, _, files in os.walk(subdir_path)])\n",
    "            #print(f'Количество изображений в {subdir}: {count}')\n",
    "            total_count += count\n",
    "    return total_count\n",
    "\n",
    "# Путь к папке train\n",
    "train_folder = './train'\n",
    "\n",
    "# Подсчет общего количества изображений во всех подпапках\n",
    "total_count_train = count_total_images(train_folder)\n",
    "total_count_val = count_total_images('./val')\n",
    "total_count_test = count_total_images('./test')\n",
    "\n",
    "# Вывод общего количества изображений\n",
    "print(f'Общее количество изображений в папке train: {total_count_train}')\n",
    "print(f'Общее количество изображений в папке val: {total_count_val}')\n",
    "print(f'Общее количество изображений в папке test: {total_count_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "495bb7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2229 images belonging to 36 classes.\n",
      "Found 443 images belonging to 36 classes.\n",
      "Epoch 1/20\n",
      "16/70 [=====>........................] - ETA: 1:13 - loss: 3.5143 - accuracy: 0.0703"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\bot\\lib\\site-packages\\PIL\\Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 112s 2s/step - loss: 2.7016 - accuracy: 0.3477 - val_loss: 1.7893 - val_accuracy: 0.5847\n",
      "Epoch 2/20\n",
      "70/70 [==============================] - 104s 1s/step - loss: 1.3053 - accuracy: 0.6990 - val_loss: 1.1132 - val_accuracy: 0.7494\n",
      "Epoch 3/20\n",
      "70/70 [==============================] - 105s 2s/step - loss: 0.8918 - accuracy: 0.7663 - val_loss: 0.9412 - val_accuracy: 0.7472\n",
      "Epoch 4/20\n",
      "70/70 [==============================] - 100s 1s/step - loss: 0.7119 - accuracy: 0.8093 - val_loss: 0.8100 - val_accuracy: 0.7856\n",
      "Epoch 5/20\n",
      "70/70 [==============================] - 98s 1s/step - loss: 0.5863 - accuracy: 0.8457 - val_loss: 0.7606 - val_accuracy: 0.7788\n",
      "Epoch 6/20\n",
      "70/70 [==============================] - 103s 1s/step - loss: 0.5108 - accuracy: 0.8537 - val_loss: 0.7255 - val_accuracy: 0.8036\n",
      "Epoch 7/20\n",
      "70/70 [==============================] - 101s 1s/step - loss: 0.4512 - accuracy: 0.8748 - val_loss: 0.7112 - val_accuracy: 0.7946\n",
      "Epoch 8/20\n",
      "70/70 [==============================] - 105s 1s/step - loss: 0.4125 - accuracy: 0.8878 - val_loss: 0.7037 - val_accuracy: 0.7878\n",
      "Epoch 9/20\n",
      "70/70 [==============================] - 102s 1s/step - loss: 0.3782 - accuracy: 0.8950 - val_loss: 0.6695 - val_accuracy: 0.8081\n",
      "Epoch 10/20\n",
      "70/70 [==============================] - 101s 1s/step - loss: 0.3210 - accuracy: 0.9139 - val_loss: 0.6619 - val_accuracy: 0.7991\n",
      "Epoch 11/20\n",
      "70/70 [==============================] - 101s 1s/step - loss: 0.2963 - accuracy: 0.9251 - val_loss: 0.6604 - val_accuracy: 0.8081\n",
      "Epoch 12/20\n",
      "70/70 [==============================] - 102s 1s/step - loss: 0.2660 - accuracy: 0.9318 - val_loss: 0.6439 - val_accuracy: 0.8081\n",
      "Epoch 13/20\n",
      "70/70 [==============================] - 102s 1s/step - loss: 0.2598 - accuracy: 0.9309 - val_loss: 0.6542 - val_accuracy: 0.8036\n",
      "Epoch 14/20\n",
      "70/70 [==============================] - 102s 1s/step - loss: 0.2397 - accuracy: 0.9394 - val_loss: 0.6554 - val_accuracy: 0.8104\n",
      "Epoch 15/20\n",
      "70/70 [==============================] - 101s 1s/step - loss: 0.2170 - accuracy: 0.9417 - val_loss: 0.6312 - val_accuracy: 0.8194\n",
      "Epoch 16/20\n",
      "70/70 [==============================] - 101s 1s/step - loss: 0.1969 - accuracy: 0.9511 - val_loss: 0.6351 - val_accuracy: 0.8284\n",
      "Epoch 17/20\n",
      "70/70 [==============================] - 101s 1s/step - loss: 0.2030 - accuracy: 0.9484 - val_loss: 0.6516 - val_accuracy: 0.8104\n",
      "Epoch 18/20\n",
      "70/70 [==============================] - 100s 1s/step - loss: 0.2008 - accuracy: 0.9453 - val_loss: 0.6648 - val_accuracy: 0.8059\n",
      "Epoch 19/20\n",
      "70/70 [==============================] - 101s 1s/step - loss: 0.1785 - accuracy: 0.9596 - val_loss: 0.6606 - val_accuracy: 0.8014\n",
      "Epoch 20/20\n",
      "70/70 [==============================] - 102s 1s/step - loss: 0.1574 - accuracy: 0.9623 - val_loss: 0.6381 - val_accuracy: 0.8149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\bot\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Установите пути к папкам данных\n",
    "train_folder = 'train'\n",
    "val_folder = 'val'\n",
    "test_folder = 'test'\n",
    "# Задайте параметры для обработки изображений\n",
    "batch_size = 32\n",
    "target_size = (224, 224)  # Выберите желаемый размер изображений\n",
    "\n",
    "# Создайте генераторы для данных обучения, валидации и тестирования\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_folder,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_folder,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Создайте и обучите модель\n",
    "base_model = MobileNetV2(input_shape=(224, 224, 3), weights='imagenet', include_top=False)\n",
    "dropout_rate = 0.5\n",
    "x = base_model.output\n",
    "x = Dropout(dropout_rate)(x)  # Добавьте слой Dropout\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "predictions = Dense(len(train_generator.class_indices), activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "# Установите желаемую скорость обучения (learning_rate)\n",
    "custom_learning_rate = 0.0001\n",
    "\n",
    "# Создайте оптимизатор с настраиваемой скоростью обучения\n",
    "custom_adam_optimizer = Adam(learning_rate=custom_learning_rate)\n",
    "\n",
    "model.compile(optimizer=custom_adam_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Установите количество эпох и обучите модель\n",
    "epochs = 20\n",
    "history = model.fit(train_generator, validation_data=val_generator, epochs=epochs)\n",
    "# Сохраните модель\n",
    "model.save('my_classification_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# График точности (accuracy) на обучающем и валидационном наборах данных\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# График функции потерь (loss) на обучающем и валидационном наборах данных\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Делаем предсказание и выводим результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbd4a21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 664ms/step\n",
      "Предсказанный класс: 6\n",
      "Предсказанный класс: carrot\n"
     ]
    }
   ],
   "source": [
    "# Загрузка названий классов из файла\n",
    "with open('French_labels copy.txt', 'r') as f:\n",
    "    class_names = f.read().splitlines()\n",
    "    \n",
    "# Загрузка обученной модели\n",
    "model = load_model('my_classification_model.h5')\n",
    "\n",
    "# Загрузка изображения для предсказания\n",
    "img = image.load_img('./морковь.jpg', target_size=(224, 224))\n",
    "\n",
    "# Преобразование изображения в массив numpy и добавление дополнительной размерности\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# Нормализация пикселей\n",
    "img_array /= 255.\n",
    "\n",
    "# Предсказание класса изображения\n",
    "prediction = model.predict(img_array)\n",
    "\n",
    "# Вывод предсказания\n",
    "print('Предсказанный класс:', np.argmax(prediction))\n",
    "print('Предсказанный класс:', class_names[np.argmax(prediction)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Cоздаем и обучаем новую модель классификации на основе предварительно обученной модели VGG16, а затем сохраняем обученную модель для дальнейшего использования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "70/70 [==============================] - 247s 4s/step - loss: 3.1927 - accuracy: 0.1615 - val_loss: 2.6391 - val_accuracy: 0.3160\n",
      "Epoch 2/10\n",
      "70/70 [==============================] - 243s 3s/step - loss: 2.3700 - accuracy: 0.3813 - val_loss: 2.1179 - val_accuracy: 0.4041\n",
      "Epoch 3/10\n",
      "70/70 [==============================] - 243s 3s/step - loss: 1.9430 - accuracy: 0.4868 - val_loss: 1.8767 - val_accuracy: 0.4740\n",
      "Epoch 4/10\n",
      "70/70 [==============================] - 245s 3s/step - loss: 1.7111 - accuracy: 0.5437 - val_loss: 1.7389 - val_accuracy: 0.4989\n",
      "Epoch 5/10\n",
      "70/70 [==============================] - 236s 3s/step - loss: 1.5129 - accuracy: 0.5868 - val_loss: 1.6866 - val_accuracy: 0.5102\n",
      "Epoch 6/10\n",
      "70/70 [==============================] - 233s 3s/step - loss: 1.4078 - accuracy: 0.6110 - val_loss: 1.5374 - val_accuracy: 0.5214\n",
      "Epoch 7/10\n",
      "70/70 [==============================] - 236s 3s/step - loss: 1.3084 - accuracy: 0.6330 - val_loss: 1.4657 - val_accuracy: 0.5576\n",
      "Epoch 8/10\n",
      "70/70 [==============================] - 240s 3s/step - loss: 1.2466 - accuracy: 0.6402 - val_loss: 1.5012 - val_accuracy: 0.5350\n",
      "Epoch 9/10\n",
      "70/70 [==============================] - 244s 3s/step - loss: 1.1796 - accuracy: 0.6649 - val_loss: 1.4698 - val_accuracy: 0.5756\n",
      "Epoch 10/10\n",
      "70/70 [==============================] - 246s 4s/step - loss: 1.1108 - accuracy: 0.6761 - val_loss: 1.4367 - val_accuracy: 0.5756\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Создайте и обучите модель\n",
    "base_model = VGG16(input_shape=(224, 224, 3), weights='imagenet', include_top=False)\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(len(train_generator.class_indices), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Установите количество эпох и обучите модель\n",
    "epochs = 10\n",
    "model.fit(train_generator, validation_data=val_generator, epochs=epochs)\n",
    "\n",
    "# Сохраните модель\n",
    "model.save('my_classification_model2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Делаем предсказание и выводим результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000292DE679790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "Предсказанный класс: banana\n"
     ]
    }
   ],
   "source": [
    "# Загрузка названий классов из файла\n",
    "with open('French_labels copy.txt', 'r') as f:\n",
    "    class_names = f.read().splitlines()\n",
    "    \n",
    "# Загрузка обученной модели VGG16\n",
    "model = load_model('my_classification_model2.h5')\n",
    "\n",
    "# Загрузка изображения для предсказания\n",
    "img = image.load_img('./банан.jpg', target_size=(224, 224))\n",
    "\n",
    "# Преобразование изображения в массив numpy и добавление дополнительной размерности\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# Нормализация пикселей\n",
    "img_array /= 255.\n",
    "\n",
    "# Предсказание класса изображения\n",
    "prediction = model.predict(img_array)\n",
    "\n",
    "# Вывод предсказания\n",
    "print('Предсказанный класс:', class_names[np.argmax(prediction)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для увеличения точности применили custom_learning_rate = 0.0001, early_stopping, model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2229 images belonging to 36 classes.\n",
      "Found 443 images belonging to 36 classes.\n",
      "Epoch 1/30\n",
      "12/70 [====>.........................] - ETA: 1:08 - loss: 3.6182 - accuracy: 0.0483"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\bot\\lib\\site-packages\\PIL\\Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 2.7320 - accuracy: 0.3544\n",
      "Epoch 1: val_loss improved from inf to 1.79547, saving model to best_model.h5\n",
      "70/70 [==============================] - 105s 1s/step - loss: 2.7320 - accuracy: 0.3544 - val_loss: 1.7955 - val_accuracy: 0.6027\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\bot\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.3235 - accuracy: 0.7061\n",
      "Epoch 2: val_loss improved from 1.79547 to 1.10442, saving model to best_model.h5\n",
      "70/70 [==============================] - 102s 1s/step - loss: 1.3235 - accuracy: 0.7061 - val_loss: 1.1044 - val_accuracy: 0.7088\n",
      "Epoch 3/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9022 - accuracy: 0.7712\n",
      "Epoch 3: val_loss improved from 1.10442 to 0.90831, saving model to best_model.h5\n",
      "70/70 [==============================] - 102s 1s/step - loss: 0.9022 - accuracy: 0.7712 - val_loss: 0.9083 - val_accuracy: 0.7517\n",
      "Epoch 4/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.7029 - accuracy: 0.8179\n",
      "Epoch 4: val_loss improved from 0.90831 to 0.80032, saving model to best_model.h5\n",
      "70/70 [==============================] - 103s 1s/step - loss: 0.7029 - accuracy: 0.8179 - val_loss: 0.8003 - val_accuracy: 0.7720\n",
      "Epoch 5/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5963 - accuracy: 0.8318\n",
      "Epoch 5: val_loss improved from 0.80032 to 0.75602, saving model to best_model.h5\n",
      "70/70 [==============================] - 103s 1s/step - loss: 0.5963 - accuracy: 0.8318 - val_loss: 0.7560 - val_accuracy: 0.7856\n",
      "Epoch 6/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5161 - accuracy: 0.8591\n",
      "Epoch 6: val_loss improved from 0.75602 to 0.71177, saving model to best_model.h5\n",
      "70/70 [==============================] - 102s 1s/step - loss: 0.5161 - accuracy: 0.8591 - val_loss: 0.7118 - val_accuracy: 0.7923\n",
      "Epoch 7/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4510 - accuracy: 0.8811\n",
      "Epoch 7: val_loss improved from 0.71177 to 0.69505, saving model to best_model.h5\n",
      "70/70 [==============================] - 102s 1s/step - loss: 0.4510 - accuracy: 0.8811 - val_loss: 0.6951 - val_accuracy: 0.7878\n",
      "Epoch 8/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4110 - accuracy: 0.8892\n",
      "Epoch 8: val_loss improved from 0.69505 to 0.67039, saving model to best_model.h5\n",
      "70/70 [==============================] - 103s 1s/step - loss: 0.4110 - accuracy: 0.8892 - val_loss: 0.6704 - val_accuracy: 0.8014\n",
      "Epoch 9/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3594 - accuracy: 0.9053\n",
      "Epoch 9: val_loss did not improve from 0.67039\n",
      "70/70 [==============================] - 100s 1s/step - loss: 0.3594 - accuracy: 0.9053 - val_loss: 0.6743 - val_accuracy: 0.7991\n",
      "Epoch 10/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3318 - accuracy: 0.9094\n",
      "Epoch 10: val_loss improved from 0.67039 to 0.65048, saving model to best_model.h5\n",
      "70/70 [==============================] - 93s 1s/step - loss: 0.3318 - accuracy: 0.9094 - val_loss: 0.6505 - val_accuracy: 0.8081\n",
      "Epoch 11/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2990 - accuracy: 0.9197\n",
      "Epoch 11: val_loss improved from 0.65048 to 0.64956, saving model to best_model.h5\n",
      "70/70 [==============================] - 94s 1s/step - loss: 0.2990 - accuracy: 0.9197 - val_loss: 0.6496 - val_accuracy: 0.8059\n",
      "Epoch 12/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2780 - accuracy: 0.9305\n",
      "Epoch 12: val_loss improved from 0.64956 to 0.63875, saving model to best_model.h5\n",
      "70/70 [==============================] - 94s 1s/step - loss: 0.2780 - accuracy: 0.9305 - val_loss: 0.6388 - val_accuracy: 0.8036\n",
      "Epoch 13/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2582 - accuracy: 0.9309\n",
      "Epoch 13: val_loss did not improve from 0.63875\n",
      "70/70 [==============================] - 93s 1s/step - loss: 0.2582 - accuracy: 0.9309 - val_loss: 0.6494 - val_accuracy: 0.8217\n",
      "Epoch 14/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2394 - accuracy: 0.9332\n",
      "Epoch 14: val_loss did not improve from 0.63875\n",
      "70/70 [==============================] - 100s 1s/step - loss: 0.2394 - accuracy: 0.9332 - val_loss: 0.6439 - val_accuracy: 0.8262\n",
      "Epoch 15/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2156 - accuracy: 0.9394\n",
      "Epoch 15: val_loss did not improve from 0.63875\n",
      "70/70 [==============================] - 103s 1s/step - loss: 0.2156 - accuracy: 0.9394 - val_loss: 0.6628 - val_accuracy: 0.8036\n",
      "Epoch 16/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2090 - accuracy: 0.9448\n",
      "Epoch 16: val_loss improved from 0.63875 to 0.63567, saving model to best_model.h5\n",
      "70/70 [==============================] - 102s 1s/step - loss: 0.2090 - accuracy: 0.9448 - val_loss: 0.6357 - val_accuracy: 0.7991\n",
      "Epoch 17/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9529\n",
      "Epoch 17: val_loss did not improve from 0.63567\n",
      "70/70 [==============================] - 101s 1s/step - loss: 0.1927 - accuracy: 0.9529 - val_loss: 0.6629 - val_accuracy: 0.8081\n",
      "Epoch 18/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1825 - accuracy: 0.9574\n",
      "Epoch 18: val_loss did not improve from 0.63567\n",
      "70/70 [==============================] - 102s 1s/step - loss: 0.1825 - accuracy: 0.9574 - val_loss: 0.6491 - val_accuracy: 0.8059\n",
      "Epoch 19/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1775 - accuracy: 0.9565\n",
      "Epoch 19: val_loss did not improve from 0.63567\n",
      "70/70 [==============================] - 102s 1s/step - loss: 0.1775 - accuracy: 0.9565 - val_loss: 0.6462 - val_accuracy: 0.8126\n",
      "Epoch 20/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1590 - accuracy: 0.9619\n",
      "Epoch 20: val_loss improved from 0.63567 to 0.63502, saving model to best_model.h5\n",
      "70/70 [==============================] - 102s 1s/step - loss: 0.1590 - accuracy: 0.9619 - val_loss: 0.6350 - val_accuracy: 0.8239\n",
      "Epoch 21/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1614 - accuracy: 0.9596\n",
      "Epoch 21: val_loss did not improve from 0.63502\n",
      "70/70 [==============================] - 102s 1s/step - loss: 0.1614 - accuracy: 0.9596 - val_loss: 0.6506 - val_accuracy: 0.8149\n",
      "Epoch 22/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1520 - accuracy: 0.9619\n",
      "Epoch 22: val_loss did not improve from 0.63502\n",
      "70/70 [==============================] - 104s 1s/step - loss: 0.1520 - accuracy: 0.9619 - val_loss: 0.6515 - val_accuracy: 0.8149\n",
      "Epoch 23/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1386 - accuracy: 0.9681\n",
      "Epoch 23: val_loss did not improve from 0.63502\n",
      "70/70 [==============================] - 102s 1s/step - loss: 0.1386 - accuracy: 0.9681 - val_loss: 0.6385 - val_accuracy: 0.8284\n",
      "Epoch 24/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1474 - accuracy: 0.9596\n",
      "Epoch 24: val_loss did not improve from 0.63502\n",
      "70/70 [==============================] - 103s 1s/step - loss: 0.1474 - accuracy: 0.9596 - val_loss: 0.6606 - val_accuracy: 0.8059\n",
      "Epoch 25/30\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1402 - accuracy: 0.9601\n",
      "Epoch 25: val_loss did not improve from 0.63502\n",
      "70/70 [==============================] - 102s 1s/step - loss: 0.1402 - accuracy: 0.9601 - val_loss: 0.6633 - val_accuracy: 0.8036\n"
     ]
    }
   ],
   "source": [
    "# Установите пути к папкам данных\n",
    "train_folder = 'train'\n",
    "val_folder = 'val'\n",
    "test_folder = 'test'\n",
    "# Задайте параметры для обработки изображений\n",
    "batch_size = 32\n",
    "target_size = (224, 224)  # Выберите желаемый размер изображений\n",
    "\n",
    "# Создайте генераторы для данных обучения, валидации и тестирования\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_folder,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_folder,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Создайте и обучите модель\n",
    "base_model = MobileNetV2(input_shape=(224, 224, 3), weights='imagenet', include_top=False)\n",
    "dropout_rate = 0.5\n",
    "x = base_model.output\n",
    "x = Dropout(dropout_rate)(x)  # Добавьте слой Dropout\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "predictions = Dense(len(train_generator.class_indices), activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "# Установите желаемую скорость обучения (learning_rate)\n",
    "custom_learning_rate = 0.0001\n",
    "\n",
    "# Создайте оптимизатор с настраиваемой скоростью обучения\n",
    "custom_adam_optimizer = Adam(learning_rate=custom_learning_rate)\n",
    "\n",
    "model.compile(optimizer=custom_adam_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Установите количество эпох и обучите модель\n",
    "epochs = 30\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# Настройка ранней остановки\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Мониторим функцию потерь на валидационном наборе данных\n",
    "    patience=5,           # Количество эпох без улучшений перед остановкой\n",
    "    restore_best_weights=True  # Восстановить веса модели до лучшей эпохи\n",
    ")\n",
    "\n",
    "# Настройка сохранения лучшей модели\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'best_model.h5',  # Имя файла для сохранения лучшей модели\n",
    "    monitor='val_loss',  # Мониторим функцию потерь на валидационном наборе данных\n",
    "    save_best_only=True,  # Сохранять только лучшую модель\n",
    "    mode='min',           # Режим мониторинга: 'min' для минимизации функции потерь\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping, model_checkpoint]  # Добавьте обратные вызовы\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Получите данные о точности и потерях из объекта history\n",
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Создайте графики\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(accuracy, label='Точность')\n",
    "plt.plot(val_accuracy, label='Точность на валидации')\n",
    "plt.legend()\n",
    "plt.title('График точности')\n",
    "plt.xlabel('Эпохи')\n",
    "plt.ylabel('Точность')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss, label='Потери')\n",
    "plt.plot(val_loss, label='Потери на валидации')\n",
    "plt.legend()\n",
    "plt.title('График потерь')\n",
    "plt.xlabel('Эпохи')\n",
    "plt.ylabel('Потери')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
